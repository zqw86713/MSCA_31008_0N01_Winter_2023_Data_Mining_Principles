{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MSCA 31008 - Data Mining Assignment 4 Part 2 (Group 4)\n",
    "<b>Qingwei Zhang, Jake Brewer, Prinu Mathew</b><br>\n",
    "<b>Winter 2023</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Libraries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json, subprocess\n",
    "\n",
    "## for data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ Already installed required packages for machine learning ~~~~\n"
     ]
    }
   ],
   "source": [
    "## for machine learning\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "    from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "    from sklearn.metrics import (\n",
    "        confusion_matrix,\n",
    "        classification_report,\n",
    "        f1_score,\n",
    "        roc_auc_score,\n",
    "        roc_curve,\n",
    "        accuracy_score,\n",
    "    )\n",
    "\n",
    "    import graphviz\n",
    "\n",
    "    print(\"~~~ Already installed required packages for machine learning ~~~~\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "    print(\"~~~ Installing required packages for machine learning ~~~~\")\n",
    "    subprocess.check_call(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"kneed\"]\n",
    "    )\n",
    "    subprocess.check_call(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"scikit-learn\"]\n",
    "    )\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "    from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "    from sklearn.metrics import (\n",
    "        confusion_matrix,\n",
    "        classification_report,\n",
    "        f1_score,\n",
    "        roc_auc_score,\n",
    "        roc_curve,\n",
    "        accuracy_score,\n",
    "    )\n",
    "\n",
    "    import graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for interactive visualization\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    print(\"~~~ Already installed required packages for interactive visualizations ~~~~\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    print(\"~~~ Installing required packages for interactive visualizations ~~~~\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"matplotlib\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"seaborn\"])\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load and Explore Data (from Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the preprocess data from part 1\n",
    "\n",
    "df = pd.read_csv(\"diabetes_data_preprocess.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view data types and number of non-null values in each column\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split Data into Training (70%) and Testing (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test using random state for reproducable results\n",
    "# exact same split as used in part 1\n",
    "\n",
    "X = df.drop(columns=[\"readmitted\"])\n",
    "y = df[\"readmitted\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "y_train.value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perform Cross Validation to find best Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run grid search cross validation to find best regularization hyper-parameters\n",
    "\n",
    "param_map = {\n",
    "    \"max_depth\": [5, 7, 9],\n",
    "    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"max_leaf_nodes\": [8, 10, 12],\n",
    "}\n",
    "\n",
    "# specify class_weight='balanced' to account for imbalance in re-admitted and non re-admitted patients\n",
    "clf = DecisionTreeClassifier(class_weight=\"balanced\", random_state=42)\n",
    "clf_gs = GridSearchCV(clf, param_grid=param_map, cv=5, n_jobs=-1, verbose=2)\n",
    "clf_gs.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    clf_gs.cv_results_,\n",
    "    columns=[\n",
    "        \"rank_test_score\",\n",
    "        \"max_depth\",\n",
    "        \"param_criterion\",\n",
    "        \"param_max_depth\",\n",
    "        \"param_max_features\",\n",
    "        \"param_max_leaf_nodes\",\n",
    "        \"mean_test_score\",\n",
    "        \"std_test_score\",\n",
    "    ],\n",
    ").sort_values(by=[\"rank_test_score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify best hyper-parameters\n",
    "\n",
    "best_criterion = clf_gs.best_params_[\"criterion\"]\n",
    "best_depth = clf_gs.best_params_[\"max_depth\"]\n",
    "best_feat = clf_gs.best_params_[\"max_features\"]\n",
    "best_nodes = clf_gs.best_params_[\"max_leaf_nodes\"]\n",
    "\n",
    "clf_gs.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify most important features\n",
    "\n",
    "best_model = clf_gs.best_estimator_\n",
    "imp_top_features = pd.DataFrame(\n",
    "    {\"Feature\": X_train.columns, \"Importance\": best_model.feature_importances_}\n",
    ")\n",
    "imp_top_features = imp_top_features[imp_top_features[\"Importance\"] > 0]\n",
    "imp_top_features_sorted = imp_top_features.sort_values(by=\"Importance\", ascending=False)\n",
    "imp_top_features_sorted\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Best Model on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use top 10 features and best hyper-parameters to fit a best model\n",
    "\n",
    "imp_top_features = imp_top_features_sorted[\"Feature\"].iloc[\n",
    "    : len(imp_top_features_sorted)\n",
    "]\n",
    "X_train = X_train[imp_top_features]\n",
    "X_test = X_test[imp_top_features]\n",
    "\n",
    "clf_trim = DecisionTreeClassifier(\n",
    "    class_weight=\"balanced\",\n",
    "    criterion=best_criterion,\n",
    "    max_depth=best_depth,\n",
    "    max_features=best_feat,\n",
    "    max_leaf_nodes=best_nodes,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "clf_trim.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the test set\n",
    "y_pred = clf_trim.predict(X_test)\n",
    "\n",
    "# evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy after cross validation:\", accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Confusion Matrix on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for training data\n",
    "\n",
    "y_pred = clf_trim.predict(X_train)\n",
    "\n",
    "# create confusion matrix and classification report\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "mat = confusion_matrix(y_train, y_pred)\n",
    "sns.heatmap(\n",
    "    mat,\n",
    "    square=True,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"coolwarm\",\n",
    "    xticklabels=[0, 1],\n",
    "    yticklabels=[0, 1],\n",
    ")\n",
    "ax.set(xlabel=\"Predicted Label\")\n",
    "ax.set(ylabel=\"True Label\")\n",
    "plt.show()\n",
    "print(classification_report(y_train, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot decision tree from Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "from six import StringIO\n",
    "\n",
    "tree_dot = StringIO()\n",
    "\n",
    "import pydotplus\n",
    "\n",
    "response = [\"Readmitted\", \"Not Readmitted\"]\n",
    "\n",
    "export_graphviz(\n",
    "    clf_trim,\n",
    "    out_file=tree_dot,\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=response,\n",
    "    rounded=True,\n",
    "    filled=True,\n",
    ")\n",
    "\n",
    "tree_graph = pydotplus.graph_from_dot_data(tree_dot.getvalue())\n",
    "Image(tree_graph.create_png())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "\n",
    "datetime.datetime.now(pytz.timezone(\"US/Central\")).strftime(\"%a, %d %B %Y %H:%M:%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmi",
   "language": "python",
   "name": "hmi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c347c8f9a7ef94e4c9e03b4513be7835ed18f45b99a2a817fb579f408b867b16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
