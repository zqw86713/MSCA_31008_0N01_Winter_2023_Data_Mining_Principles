{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MSCA 31008 - Data Mining Assignment 4 Part 1 (Group 4)\n",
    "<b>Qingwei Zhang, Jake Brewer, Prinu Mathew</b><br>\n",
    "<b>Winter 2023</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Libraries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json, subprocess\n",
    "\n",
    "## for data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "## for machine learning\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for machine learning\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import (\n",
    "        confusion_matrix,\n",
    "        classification_report,\n",
    "        f1_score,\n",
    "        roc_auc_score,\n",
    "        roc_curve,\n",
    "        accuracy_score,\n",
    "    )\n",
    "    import sklearn.datasets\n",
    "\n",
    "    print(\"~~~ Already installed required packages for machine learning ~~~~\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "    print(\"~~~ Installing required packages for machine learning ~~~~\")\n",
    "    subprocess.check_call(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"kneed\"]\n",
    "    )\n",
    "    subprocess.check_call(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"scikit-learn\"]\n",
    "    )\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import (\n",
    "        confusion_matrix,\n",
    "        classification_report,\n",
    "        f1_score,\n",
    "        roc_auc_score,\n",
    "        roc_curve,\n",
    "        accuracy_score,\n",
    "    )\n",
    "    import sklearn.datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for interactive visualization\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    print(\"~~~ Already installed required packages for interactive visualizations ~~~~\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    print(\"~~~ Installing required packages for interactive visualizations ~~~~\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"matplotlib\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"seaborn\"])\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data and read csv\n",
    "\n",
    "input_df = pd.read_csv(\"diabetes_data.csv\")\n",
    "input_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view data types and number of non-null values in each column\n",
    "input_df.info()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Input into Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 2 value categories into binary variables\n",
    "input_df[\"readmitted\"] = input_df[\"readmitted\"].replace(\">30\", \"YES\")\n",
    "input_df[\"readmitted\"] = input_df[\"readmitted\"].replace(\"<30\", \"YES\")\n",
    "input_df[\"readmitted\"] = input_df[\"readmitted\"].replace(\"YES\", 1)\n",
    "input_df[\"readmitted\"] = input_df[\"readmitted\"].replace(\"NO\", 0)\n",
    "input_df[\"readmitted\"] = input_df[\"readmitted\"].astype(int)\n",
    "input_df[\"readmitted\"].value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### diag_1, diag_2, diag_3 variables : \n",
    "\n",
    "The dataset contained upto three diagnoses for a given patient (primary (diag_1), secondary(diag_2) and additional(diag_3)). However, each of these had 700â€“900 unique ICD codes  \n",
    "We can collapsed these diagnosis codes into 9 disease categories . These 9 categories include Circulatory, Respiratory, Digestive, Diabetes, Injury, Musculoskeletal, Genitourinary, Neoplasms, and Others. \n",
    "\n",
    "* You can convert all 3 into 9 categories each. however primary diagnosis is enough for this study.\n",
    "\n",
    "ICD9 code conversion online reference (https://en.wikipedia.org/wiki/List_of_ICD-9_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recategorize diagnoses into 9 categories - only consider primary diagnosis\n",
    "def convert_diag_codes(code):\n",
    "    if pd.isnull(code):\n",
    "        return \"Other\"\n",
    "    elif (\"V\" in code) or (\"E\" in code):\n",
    "        return \"Other\"\n",
    "    else:\n",
    "        code = float(code)\n",
    "        if (code >= 390) and (code < 460) or (np.floor(code) == 785):\n",
    "            return \"Circulatory\"\n",
    "        elif (code >= 460) and (code < 520) or (np.floor(code) == 786):\n",
    "            return \"Respiratory\"\n",
    "        elif (code >= 520) and (code < 580) or (np.floor(code) == 787):\n",
    "            return \"Digestive\"\n",
    "        elif code == 250:\n",
    "            return \"Diabetes\"\n",
    "        elif (code >= 800) and (code < 1000):\n",
    "            return \"Injury\"\n",
    "        elif (code >= 710) and (code < 740):\n",
    "            return \"Musculoskeletal\"\n",
    "        elif (code >= 580) and (code < 630) or (np.floor(code) == 788):\n",
    "            return \"Genitourinary\"\n",
    "        elif (code >= 140) and (code < 240):\n",
    "            return \"Neoplasms\"\n",
    "        else:\n",
    "            return \"Other\"\n",
    "\n",
    "\n",
    "input_df[\"diag_1\"] = input_df[\"diag_1\"].apply(lambda x: convert_diag_codes(x))\n",
    "input_df[\"diag_1\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert age to numeric\n",
    "age_dict = {\n",
    "    \"[0-10)\": 5,\n",
    "    \"[10-20)\": 15,\n",
    "    \"[20-30)\": 25,\n",
    "    \"[30-40)\": 35,\n",
    "    \"[40-50)\": 45,\n",
    "    \"[50-60)\": 55,\n",
    "    \"[60-70)\": 65,\n",
    "    \"[70-80)\": 75,\n",
    "    \"[80-90)\": 85,\n",
    "    \"[90-100)\": 95,\n",
    "}\n",
    "\n",
    "input_df[\"age\"] = input_df[\"age\"].map(age_dict)\n",
    "input_df[\"age\"] = input_df[\"age\"].astype(\"int64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 2 value categories into binary variables\n",
    "input_df[\"change\"] = input_df[\"change\"].replace(\"Ch\", 1)\n",
    "input_df[\"change\"] = input_df[\"change\"].replace(\"No\", 0)\n",
    "input_df[\"gender\"] = input_df[\"gender\"].replace(\"Male\", 1)\n",
    "input_df[\"gender\"] = input_df[\"gender\"].replace(\"Female\", 0)\n",
    "input_df[\"diabetesMed\"] = input_df[\"diabetesMed\"].replace(\"Yes\", 1)\n",
    "input_df[\"diabetesMed\"] = input_df[\"diabetesMed\"].replace(\"No\", 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge categories to 1 - 2 levels using domain knowledge\n",
    "input_df[\"A1Cresult\"] = input_df[\"A1Cresult\"].replace(\">7\", \"Abnormal\")\n",
    "input_df[\"A1Cresult\"] = input_df[\"A1Cresult\"].replace(\">8\", \"Abnormal\")\n",
    "input_df[\"A1Cresult\"] = input_df[\"A1Cresult\"].replace(\"Norm\", \"Normal\")\n",
    "input_df[\"A1Cresult\"] = input_df[\"A1Cresult\"].replace(\"None\", \"Not tested\")\n",
    "input_df[\"max_glu_serum\"] = input_df[\"max_glu_serum\"].replace(\">200\", \"Abnormal\")\n",
    "input_df[\"max_glu_serum\"] = input_df[\"max_glu_serum\"].replace(\">300\", \"Abnormal\")\n",
    "input_df[\"max_glu_serum\"] = input_df[\"max_glu_serum\"].replace(\"Norm\", \"Normal\")\n",
    "input_df[\"max_glu_serum\"] = input_df[\"max_glu_serum\"].replace(\"None\", \"Not tested\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge some categories together\n",
    "input_df[\"admission_type_id\"] = input_df[\"admission_type_id\"].replace(2, 1)\n",
    "input_df[\"admission_type_id\"] = input_df[\"admission_type_id\"].replace(7, 1)\n",
    "input_df[\"admission_type_id\"] = input_df[\"admission_type_id\"].replace(6, 5)\n",
    "input_df[\"admission_type_id\"] = input_df[\"admission_type_id\"].replace(8, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numerical look-a-likes to string\n",
    "input_df[\"admission_type_id\"] = input_df[\"admission_type_id\"].map(str)\n",
    "input_df[\"discharge_disposition_id\"] = input_df[\"discharge_disposition_id\"].map(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate patients\n",
    "input_df = input_df.drop_duplicates(subset=[\"patient_nbr\"], keep=\"first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop/remove unnecessary variables\n",
    "input_df.drop(\n",
    "    [\"patient_nbr\", \"diag_2\", \"diag_3\", \"encounter_id\", \"admission_source_id\"],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing value for race\n",
    "input_df[\"race\"].replace(np.nan, \"Missing\", inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply one-hot encoding\n",
    "one_hot_list = [col for col in input_df.columns if input_df[col].dtype == \"object\"]\n",
    "data_preprocess = pd.get_dummies(input_df, columns=one_hot_list, drop_first=True)\n",
    "data_preprocess.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to csv for future use in part 2\n",
    "data_preprocess.to_csv(\"diabetes_data_preprocess.csv\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre-processing the data we are left with 102 columns (original dataset had 44) and data on 68,630 unique patients (removed duplicate patient data, keeping first entry)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split Data into Training (70%) and Testing (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into features and target and using random state so that results are reproducible in part 2\n",
    "\n",
    "X = data_preprocess.drop(columns=[\"readmitted\"])\n",
    "y = data_preprocess[\"readmitted\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "y_train.value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model on All Variables to Find Most Important"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Use sklearn.linear_model.LogisticRegression OR discrete.discrete_model\n",
    "-   Use cross validation, parameter tuning (penalty, c) using GridSearchCV\n",
    "-   Select best variables by looking at coefficients of variables and fit model with best variables and best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify class_weight='balanced' to account for imbalance in re-admitted and non re-admitted patients\n",
    "clf = LogisticRegression(class_weight=\"balanced\", random_state=42, max_iter=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Actual Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run grid search cross validation to find best regularization hyper-parameters\n",
    "param_map = {\"penalty\": [\"l1\", \"l2\"], \"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "clf_gs = GridSearchCV(clf, param_grid=param_map, cv=5, verbose=1)\n",
    "clf_gs.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    clf_gs.cv_results_,\n",
    "    columns=[\n",
    "        \"rank_test_score\",\n",
    "        \"param_C\",\n",
    "        \"param_penalty\",\n",
    "        \"mean_test_score\",\n",
    "        \"std_test_score\",\n",
    "    ],\n",
    ").sort_values(by=[\"rank_test_score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify best hyper-parameters\n",
    "best_penalty = clf_gs.best_params_[\"penalty\"]\n",
    "best_c = clf_gs.best_params_[\"C\"]\n",
    "clf_gs.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best estimator from the cross validation to identify the most important features based on their coefficients\n",
    "best_model = clf_gs.best_estimator_\n",
    "coef_df = pd.DataFrame(\n",
    "    {\"Variable\": X_train.columns, \"Coefficient\": best_model.coef_.flatten()}\n",
    ")\n",
    "coef_df[\"Abs Coefficient\"] = abs(coef_df[\"Coefficient\"])\n",
    "sorted_coef_df = coef_df.sort_values(by=\"Abs Coefficient\", ascending=False)\n",
    "sorted_coef_df.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the top 40 features with highest coefficient\n",
    "best_coefs = sorted_coef_df[\"Variable\"].iloc[:40]\n",
    "X_train_best_features = X_train[best_coefs]\n",
    "X_test_best_features = X_test[best_coefs]\n",
    "\n",
    "X_train_best_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train new model with best features\n",
    "clf_trim = LogisticRegression(\n",
    "    class_weight=\"balanced\",\n",
    "    C=best_c,\n",
    "    penalty=best_penalty,\n",
    "    random_state=42,\n",
    "    max_iter=100,\n",
    ")\n",
    "clf_trim.fit(X_train_best_features, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the test set\n",
    "y_pred = clf_trim.predict(X_test_best_features)\n",
    "\n",
    "# evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy after cross validation:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_train, clf.predict(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_train, best_model.predict(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_train, clf_trim.predict(X_train_best_features))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: A good F1 score (the harmonic mean of precision and recall) means that you have low false positives and low false negatives, so you're correctly identifying real threats and you are not disturbed by false alarms. An F1 score is considered perfect when it's 1 , while the model is a total failure when it's 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the results of model with all features to model with fewer features\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax[0].bar(\n",
    "    [\"All Predictors\", \"Best Predictors\"],\n",
    "    [\n",
    "        best_model.score(X_train, y_train),\n",
    "        clf_trim.score(X_train_best_features, y_train),\n",
    "    ],\n",
    "    color=[\"b\", \"r\"],\n",
    ")\n",
    "\n",
    "ax[1].bar(\n",
    "    [\"All Predictors\", \"Best Predictors\"],\n",
    "    [\n",
    "        f1_score(y_train, best_model.predict(X_train)),\n",
    "        f1_score(y_train, clf_trim.predict(X_train_best_features)),\n",
    "    ],\n",
    "    color=[\"b\", \"r\"],\n",
    ")\n",
    "\n",
    "ax[0].set(title=\"Accuracy\", ylabel=\"Score\")\n",
    "ax[1].set(title=\"F1-Score\", ylabel=\"Score\")\n",
    "\n",
    "print(f\"All Predictor Variables:  {len(best_model.coef_[0, :])}\")\n",
    "print(f\"Best Predictor Variables: {len(clf_trim.coef_[0, :])}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running grid search cross validation we identified the best C hyper-tuning parameter to be 10 with l2 regularization. Using these hyper-tuning parameters we identified the top 40 features that had the most significant impact on the model based on their coefficients. We re-trained the logistic regression model using only these 40 features and compared the accuracy and F1-score to that of the original model with 101 variables. The accuracy of the second model on the trained data is about the same as the accuracy of the original model, while the F1-score of the second model is about .02 higher than the F1-score of the original model. Due to this slight increase in performance we will stick with the second model (40 features) and moving forward."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Model Performance on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for training data\n",
    "\n",
    "y_pred = clf_trim.predict(X_train_best_features)\n",
    "# y_pred = best_model.predict(X_train)\n",
    "\n",
    "# create confusion matrix and classification report\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "mat = confusion_matrix(y_train, y_pred)\n",
    "sns.heatmap(\n",
    "    mat,\n",
    "    square=True,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"coolwarm\",\n",
    "    xticklabels=[0, 1],\n",
    "    yticklabels=[0, 1],\n",
    ")\n",
    "ax.set(xlabel=\"Predicted Label\")\n",
    "ax.set(ylabel=\"True Label\")\n",
    "print(\"Threshold = 0.5\")\n",
    "plt.show()\n",
    "print(classification_report(y_train, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.where(clf_trim.predict_proba(X_train_best_features)[:, 1] > 0.4, 1, 0)\n",
    "# y_pred = np.where(best_model.predict_proba(X_train)[:,1] > .4, 1, 0)\n",
    "\n",
    "# create confusion matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "mat = confusion_matrix(y_train, y_pred)\n",
    "sns.heatmap(\n",
    "    mat,\n",
    "    square=True,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"coolwarm\",\n",
    "    xticklabels=[0, 1],\n",
    "    yticklabels=[0, 1],\n",
    ")\n",
    "ax.set(xlabel=\"Predicted Label\")\n",
    "ax.set(ylabel=\"True Label\")\n",
    "print(\"Threshold = 0.4\")\n",
    "plt.show()\n",
    "print(classification_report(y_train, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.where(clf_trim.predict_proba(X_train_best_features)[:, 1] > 0.6, 1, 0)\n",
    "# y_pred = np.where(best_model.predict_proba(X_train)[:,1] > .6, 1, 0)\n",
    "\n",
    "# create confusion matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "mat = confusion_matrix(y_train, y_pred)\n",
    "sns.heatmap(\n",
    "    mat,\n",
    "    square=True,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"coolwarm\",\n",
    "    xticklabels=[0, 1],\n",
    "    yticklabels=[0, 1],\n",
    ")\n",
    "ax.set(xlabel=\"Predicted Label\")\n",
    "ax.set(ylabel=\"True Label\")\n",
    "print(\"Threshold = 0.6\")\n",
    "plt.show()\n",
    "print(classification_report(y_train, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing model performance on the training data at various threshold, it appears that a 0.6 threshold yields a slightly higher accuracy than the default 0.5 threshold:\n",
    "- Threshold: 0.5 Accuracy: .60\n",
    "- Threshold: 0.4 Accuracy: .50\n",
    "- Threshold: 0.6 Accuracy: .63\n",
    "\n",
    "But at the cost of a much lower F1-score:\n",
    "- Threshold: 0.5 F1-Score: .54\n",
    "- Threshold: 0.4 F1-Score: .59\n",
    "- Threshold: 0.6 F1-Score: .33\n",
    "\n",
    "May be business analyst who has domain knowledge could determine which is more important here - catching more at-risk patients for re-admission at the cost of a larger false positive rate, or maximizing overall accuracy. Given the slight decline in accuracy for a much higher F1-score though, we will consider the 0.5 threshold to be the best performer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Model Performance on Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for testing data\n",
    "\n",
    "y_pred = clf_trim.predict(X_test_best_features)\n",
    "\n",
    "# create confusion matrix and classification report\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(\n",
    "    mat,\n",
    "    square=True,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"coolwarm\",\n",
    "    xticklabels=[0, 1],\n",
    "    yticklabels=[0, 1],\n",
    ")\n",
    "ax.set(xlabel=\"Predicted Label\")\n",
    "ax.set(ylabel=\"True Label\")\n",
    "plt.show()\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trimmed model is more balanced between precision and recall. However the positive class (readmitted=1) still suffers from poor performance. Namely, there is only a 50% (precision=0.50) chance that the patient actually needs to be readmitted when the model says the patient should be admitted. Perhaps it's better that recall is higher than precision because it's better to be safe than sorry and be better at identifying positive cases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Plot ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ROC-AUC curve\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "fpr, tpr, thresholds = roc_curve(\n",
    "    y_test, clf_trim.predict_proba(X_test_best_features)[:, 1]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "ax.plot(fpr, tpr, label=f\"Logistic Regression Area: {roc_auc.round(4)}\")\n",
    "ax.plot([0, 1], [0, 1], \"r--\")\n",
    "ax.set(\n",
    "    xlim=[0.0, 1.0],\n",
    "    ylim=[0.0, 1.0],\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Receiver Operating Characteristic\",\n",
    ")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the ROC curve, our model is slightly better than random chance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summarize Results\n",
    "\n",
    "### Performance on Test Dataset\n",
    "The accuracy of the logistic regression model on the test data is 60% with a .53 F1-score. The breakdown of readmitted patients on the total dataset is 40.7% readmitted and 59.3% not readmitted. Therefore the results of our logistic regression model are barely better than simply guessing that all patients will not have to be readmitted (59.3% accuracy) and it is a better than randomly guessing (50% accuracy) as seen in the ROC-AUC graph.\n",
    "\n",
    "By specifying weight class imbalance when training our model we were greatly able to even out the errors between false positives and false negatives as opposed to overwehlmingly guessing the majority class and having a higher false negative rate. This led to a higher F1-score, but did not have much of an impact on accuracy.\n",
    "\n",
    "### Model Performance on Train vs Test Datasets\n",
    "A positive for the model is that in all scenarios the training results and testing results were very close together, implying that the model is likely not overfitting, but might be underfitting. We ran grid search on 2 regularization hyper-parameters (C and penalty) which both decrease the chance that the model will overfit the training data. The confusion matrices and classification reports for the training data and testing data was very similar implying stability in the model.\n",
    "\n",
    "### Final Thoughts\n",
    "Despite a decent amount of pre-processing, the logistic regression model performed quite poorly on predicting which patients are at risk of re-admission. Additional feature engineering with the help of domain expertise could possibly improve accuracy, but most likely logistic regression is not the right model for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "\n",
    "datetime.datetime.now(pytz.timezone(\"US/Central\")).strftime(\"%a, %d %B %Y %H:%M:%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmi",
   "language": "python",
   "name": "hmi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c347c8f9a7ef94e4c9e03b4513be7835ed18f45b99a2a817fb579f408b867b16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
